{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f22cac2b-c342-45e8-a3fd-d477996ca696",
          "showTitle": false,
          "title": ""
        },
        "id": "MvTrjH-2afNe"
      },
      "source": [
        "# Value at risk - market factors\n",
        "\n",
        "In this notebook, we show how to\n",
        "- train a linear regression model using historical market factor data for each instrument in portfolio\n",
        "- ensure our market factors are not correlated and exhibit a normal distribution of market returns\n",
        "- register all models to mlflow\n",
        "- explore the use of pyfunc format\n",
        "\n",
        "This notebook requires the following dependencies\n",
        "- `mlflow`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "7968e63d-2b22-4119-a21d-8843ad8a5b3c",
          "showTitle": false,
          "title": ""
        },
        "id": "-aLeBaKHafNf"
      },
      "source": [
        "# `STEP0` Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "716dc650-d392-43b8-bef4-e1dd59a13983",
          "showTitle": true,
          "title": "Import libraries"
        },
        "id": "rYy9hiupafNf"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import math\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from numpy import savetxt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d1b8b581-d6bb-4475-9a51-3d7fc5e538c4",
          "showTitle": true,
          "title": "Control parameters"
        },
        "id": "XE-6mOP-afNg"
      },
      "outputs": [],
      "source": [
        "portfolio_table = 'var_portfolio'\n",
        "stock_table = 'var_stock'\n",
        "stock_return_table = 'var_stock_return'\n",
        "market_table = 'var_market'\n",
        "market_return_table = 'var_market_return'\n",
        "trial_table = 'var_monte_carlo'\n",
        "\n",
        "# when do we train model\n",
        "today_str = \"2019-06-01\"\n",
        "today = F.to_date(F.lit(today_str)).cast(TimestampType())\n",
        "mlflow.log_param('to_date', today_str)\n",
        "\n",
        "# our predictive market factors\n",
        "feature_names = ['SP500', 'NYSE', 'OIL', 'TREASURY', 'DOWJONES']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a0dbf238-343e-44d7-a629-a491417e2ef5",
          "showTitle": false,
          "title": ""
        },
        "id": "_5fWjQyhafNg"
      },
      "source": [
        "# `STEP1` Access data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "collapsed": true,
          "inputWidgets": {},
          "nuid": "b7a3a354-d738-4213-b4aa-bf34e2b5a033",
          "showTitle": true,
          "title": "Delta lake ensures full reproducibility"
        },
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "F4ubAU4DafNg"
      },
      "outputs": [],
      "source": [
        "versions_m_df = sql(\"DESCRIBE HISTORY \" + market_return_table).select(\"version\")\n",
        "delta_m_version = versions_m_df.toPandas()['version'].max()\n",
        "\n",
        "versions_s_df = sql(\"DESCRIBE HISTORY \" + stock_return_table).select(\"version\")\n",
        "delta_s_version = versions_s_df.toPandas()['version'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e547a392-47ac-411d-835c-38844f6abc55",
          "showTitle": true,
          "title": "Get stocks and market factor returns"
        },
        "id": "KcmQ7ASQafNh",
        "outputId": "82ec9817-5eb6-484e-f3e0-ab81c742967e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# retrieve historical tick data up to specified date\n",
        "f_ret = spark.table(market_return_table).filter(F.col('date') <= today)\n",
        "s_ret = spark.table(stock_return_table).filter(F.col('date') <= today)\n",
        "\n",
        "# market factors easily fit in memory and are required to build normal distribution of returns\n",
        "f_ret_pdf = f_ret.toPandas()\n",
        "f_ret_pdf.index = f_ret_pdf['date']\n",
        "f_ret_pdf = f_ret_pdf.drop(['date'], axis=1)\n",
        "mlflow.log_metric('x_size', f_ret_pdf.size)\n",
        "f_ret_pdf.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "4f1feea4-4bfc-4229-a946-aaaf4a248a98",
          "showTitle": false,
          "title": ""
        },
        "id": "eNBWWMoRafNh"
      },
      "source": [
        "#`STEP2` Evaluate market factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "08f6f75b-ed47-441d-9917-48559151e19b",
          "showTitle": true,
          "title": "Correlation of market factors"
        },
        "id": "zIHZnhJKafNh",
        "outputId": "a457aff8-b1dc-4643-fc53-e0e38bc9d68c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# we simply plot correlation matrix via pandas (market factors fit in memory)\n",
        "# we assume market factors are not correlated (NASDAQ and SP500 are, so are OIL and TREASURY BONDS)\n",
        "f_cor_pdf = f_ret_pdf.corr(method='spearman', min_periods=12)\n",
        "sns.set(rc={'figure.figsize':(11,8)})\n",
        "sns.heatmap(f_cor_pdf, annot=True)\n",
        "plt.savefig('/tmp/factor_correlation.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "bf4cb57c-af49-42ee-924a-385aafbcbd72",
          "showTitle": false,
          "title": ""
        },
        "id": "yhfH956FafNi"
      },
      "source": [
        "#`STEP3` Train a model for each instrument"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6d685584-a74d-4a6f-ac67-09db873393ea",
          "showTitle": true,
          "title": "Creating training dataset based on market factors"
        },
        "id": "YnFrSVm1afNi",
        "outputId": "0cb4a3ed-323a-40b2-b908-ca11cb913770"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# create our feature set based on market factors and actual portfolio return\n",
        "# in real life, we should obviously split set into training / testing\n",
        "x_train = f_ret \\\n",
        "  .withColumn(\"features\", F.array(feature_names)) \\\n",
        "  .dropna() \\\n",
        "  .select('date', 'features') \\\n",
        "  .join(s_ret, 'date')\n",
        "\n",
        "display(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "980c36a1-e3bf-44de-9751-2ade354da0fb",
          "showTitle": true,
          "title": "Train models in parallel using pandas UDF"
        },
        "id": "NR7awosNafNi",
        "outputId": "0876d404-4218-4765-88c6-0c68c456327a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# add non linear transformations as simple example on non linear returns\n",
        "def featurize(xs):\n",
        "  fs = []\n",
        "  for x in xs:\n",
        "    fs.append(x)\n",
        "    fs.append(np.sign(x) * x**2)\n",
        "    fs.append(x**3)\n",
        "    fs.append(np.sign(x) * np.sqrt(abs(x)))\n",
        "  return fs\n",
        "\n",
        "# use pandas UDF to train multiple model (one for each instrument) in parallel\n",
        "# the resulting dataframe will be the linear regression weights for each instrument\n",
        "schema = StructType([StructField('ticker', StringType(), True), StructField('weights', ArrayType(FloatType()), True)])\n",
        "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
        "def train_model(pdf):\n",
        "  # build market factor vectors\n",
        "  # add a constant - the intercept term for each instrument i.\n",
        "  X = [featurize(row) for row in np.array(pdf['features'])]\n",
        "  X = sm.add_constant(X, prepend=True)\n",
        "  y = np.array(pdf['return'])\n",
        "  model = sm.OLS(y, X).fit()\n",
        "  w_df = pd.DataFrame(data=[[model.params]], columns=['weights'])\n",
        "  w_df['ticker'] = pdf['ticker'].iloc(0)[0]\n",
        "  return w_df\n",
        "\n",
        "# the resulting dataframe easily fits in memory and will be saved as our \"uber model\", serialized to json\n",
        "models_df = x_train.groupBy('ticker').apply(train_model).toPandas()\n",
        "models_df.to_json(\"/tmp/models.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "678331dd-3dda-44ca-a691-99042345fd47",
          "showTitle": true,
          "title": "Predict daily returns"
        },
        "id": "dRR4hDt7afNi",
        "outputId": "623d2bd4-08c8-4936-c274-a468df0d5e94"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "# simply applying weight to each market factor feature\n",
        "@udf(\"float\")\n",
        "def predict_udf(xs, ps):\n",
        "  fs = featurize(xs)\n",
        "  s = ps[0]\n",
        "  for i, f in enumerate(fs):\n",
        "    s = s + ps[i + 1] * f\n",
        "  return float(s)\n",
        "\n",
        "# we read models created at previous step\n",
        "models_df = spark.createDataFrame(pd.read_json(\"/tmp/models.json\"))\n",
        "\n",
        "# we join model for each return to compute prediction of return vs. actual\n",
        "prediction_df = x_train.join(models_df, ['ticker']) \\\n",
        "  .withColumn(\"predicted\", predict_udf(F.col('features'), F.col('weights'))) \\\n",
        "  .withColumnRenamed('return', 'actual') \\\n",
        "  .select('ticker', 'date', 'predicted', 'actual')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "d40ae44b-bb68-43bf-9584-d884b1f72b30",
          "showTitle": true,
          "title": "Compute mean square error"
        },
        "id": "O3bzOc5UafNi",
        "outputId": "7a0be114-febf-4d42-cf1d-2989a1a86dcc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "@udf(\"float\")\n",
        "def wsse_udf(p, a):\n",
        "  return float((p - a)**2)\n",
        "\n",
        "# compare expected vs. actual return\n",
        "# sum mean square error per instrument\n",
        "wsse_df = prediction_df \\\n",
        "  .withColumn('wsse', wsse_udf(F.col('predicted'), F.col('actual'))) \\\n",
        "  .groupBy('ticker') \\\n",
        "  .agg(F.sum('wsse')) \\\n",
        "  .toPandas()\n",
        "\n",
        "# plot mean square error as accuracy of our model for each instrument\n",
        "ax=wsse_df.plot.bar(x='ticker', y='sum(wsse)', rot=0, label=None, figsize=(24,5))\n",
        "ax.get_legend().remove()\n",
        "plt.title(\"Model WSSE for each instrument\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel(\"wsse\")\n",
        "plt.savefig(\"/tmp/model_wsse.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ee0bc831-7893-46ef-b860-9127d9d4496b",
          "showTitle": true,
          "title": "Show predictive value for [Ecopetrol S.A.], Oil & Gas Producers in Columbia"
        },
        "id": "cb9qHla6afNi",
        "outputId": "482d7cf9-678a-4108-cc2c-594d9d5ec533"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "df = prediction_df.filter(F.col('ticker') == \"EC\").toPandas()\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(df.date, df.actual)\n",
        "plt.plot(df.date, df.predicted, color='green', linestyle='--')\n",
        "plt.title('Log return of EC')\n",
        "plt.ylabel('log return')\n",
        "plt.xlabel('date')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ed8c9698-844e-40b3-bd6d-2339be52fa3d",
          "showTitle": false,
          "title": ""
        },
        "id": "rZYpzmR3afNi"
      },
      "source": [
        "# `STEP4` register model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cbe5074a-8fae-46c0-9820-ac23378a382d",
          "showTitle": true,
          "title": "Log model and artifacts to MLflow"
        },
        "id": "PcoypsELafNi",
        "outputId": "2df60e7c-e319-43b2-ec35-2390625130f0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "mlflow.log_param('delta.version.market', delta_m_version)\n",
        "mlflow.log_param('delta.version.stocks', delta_s_version)\n",
        "mlflow.log_artifact('/tmp/model_wsse.png')\n",
        "mlflow.log_artifact('/tmp/factor_correlation.png')\n",
        "mlflow.log_artifact('/tmp/models.json')\n",
        "mlflow.end_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "8f51d27c-ee0e-423c-b551-4ff448d49da9",
          "showTitle": false,
          "title": ""
        },
        "id": "92u9po6_afNi"
      },
      "source": [
        "# `HOMEWORK` package model\n",
        "We show how any function or model can be easily wrapped as a `mlflow.pyfunc` model and registered as such on ml registry. Real life VAR models are obviously more complex than a simple linear regression described here and are not necessarily out of the box sklearn or keras models. Still, they should follow same ML development standard and can easily benefit from ml-flow functionalities as long as one can express model I/O as a form of `pd.Series`, `pd.DataFrame` or `np.array`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "da2a1e0d-75f1-412f-9218-ab28be44ff39",
          "showTitle": true,
          "title": "Package model as pyfunc"
        },
        "id": "cccfBqtBafNi",
        "outputId": "5bb074ef-4587-4c01-d9a1-0c752f9c1162"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "",
              "errorSummary": "Command skipped",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "import mlflow.pyfunc\n",
        "\n",
        "class ModelRisk(mlflow.pyfunc.PythonModel):\n",
        "\n",
        "  def load_context(self, context):\n",
        "    # can load data from ml-flow context['data']\n",
        "    print('TODO')\n",
        "\n",
        "  def predict(self, context, input):\n",
        "    # input can be a pd.Series, pd.DataFrame, np.array\n",
        "    print('TODO')\n",
        "\n",
        "artifacts = {\"data\": '/tmp/models.json'}\n",
        "model_risk = ModelRisk()\n",
        "mlflow.pyfunc.log_model(artifact_path = 'model', artifacts = artifacts, python_model=model_risk)\n",
        "mlflow.end_run()"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "02_model",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}